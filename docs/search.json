[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Here’s where I describe what I do professionally"
  },
  {
    "objectID": "about.html#what-i-do-for-work",
    "href": "about.html#what-i-do-for-work",
    "title": "About",
    "section": "",
    "text": "Here’s where I describe what I do professionally"
  },
  {
    "objectID": "about.html#what-i-do-for-fun",
    "href": "about.html#what-i-do-for-fun",
    "title": "About",
    "section": "What I do for fun",
    "text": "What I do for fun\nHere’s what I do for fun"
  },
  {
    "objectID": "posts/2025-12-01-eds222/index.html",
    "href": "posts/2025-12-01-eds222/index.html",
    "title": "Herbarium Specimen Phenology",
    "section": "",
    "text": "Many studies have found climate-related delays in the timing of leaf coloration, but do not have a lot of data from before the late 20th century (Garretson and Forkner 2021b).\nA data set collected by Garretson and Forkner in 2021 from the Southeast Regional Network of Expertise and Collections (SERNEC n.d.) contains evaluations of 2,972 digitized herbaria specimens of red and sugar maples collected between 1826 and 2016. They paired these specimens with climate and locality information to investigate long-term trends in autumn phenology over the past century (Garretson and Forkner 2021a).\nUsing this data, I want to analyze the effects of day of year (doy), year, and mean fall temperature on the presence of colored leaves as well as determine whether the day of the year when there becomes a higher probability of leaf color is shifting earlier over time.\n\n\n\n\nDAG\n\n\n\nI think that year, day of the year, and mean fall temperature all have an effect on whether a leaf is colored. Day of year is a seasonal relationship, where leaf coloration is expected the least in the summer (~180-250 doy) and more after that period in the fall (~250-300 doy). Cooler fall temperatures could cause a quicker onset of leaf coloration and senescence, while warmer mean temperatures could delay it. Year could affect leaf coloration through long-term trends in climate, and could also directly affect mean fall temperatures, as some years are generally warmer or cooler than others.\n\n\n\n\nCode\nlibrary(here)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(segmented)\nlibrary(kableExtra)\nlibrary(broom)\nlibrary(gtsummary)\nlibrary(modelsummary)\n\n# Read in data\nautumn_data &lt;- read_csv(here::here(\"posts\", \"2025-12-01-eds222\", \"data\", \"autumn_data.csv\")) %&gt;% \n  clean_names() \n\n\n\n\n\n\n\nCode\nggplot(autumn_data, aes(x = longitude, y = latitude)) +\n  geom_point(color = \"darkgreen\") +\n  labs(x = \"Longitude\", y = \"Latitude\", \n       title = \"Herbarium Specimen Locations\") +\n  theme_minimal()\n\n\n\n\n\nFigure 1: Initial exploration of the spatial distribution of Herbarium specimens.\n\n\n\n\nThe specimens are located relatively uniformly throughout the Northeast.\n\n\nCode\n# Create scatter plot\nggplot(autumn_data, aes(x = doy, y = colored_leaves, color = species)) +\n  geom_jitter(alpha = 0.5, width = 0, height = 0.1) +\n  scale_color_manual(values = c(\"#6F8FAF\", \"#8A9A5B\")) +\n  labs(x = \"Day of Year\",\n       y = \"Colored Leaves\") +\n  theme_minimal() \n\n\n\n\n\nFigure 2: Initial exploration of the effects of day of the year on the presence of colored leaves (0/1).\n\n\n\n\nThe binary colored_leaves variable appears to have many more specimens with non-colored leaves, which are spread throughout the entire year but show a cluster between about doy 80 and 300. There are fewer specimens with colored leaves, with a more concentrated spread between doy 100 and 320. This data distribution suggests that a logistic regression model could be a good fit.\nA better way to visualize the colored_leaves variable and potentially see a seasonal trend is to group our observations into 2-week bins and calculate the proportion of colored leaves within each bin.\n\n\nCode\n# Create 14-day bins\nautumn_binned &lt;- autumn_data %&gt;%\n  mutate(doy_bin = cut(doy, breaks = seq(0, 366, by = 14), include.lowest = TRUE, right = FALSE)) %&gt;%\n  group_by(doy_bin) %&gt;%\n  summarize(n = n(),  # total number within each bin\n            prop_colored = mean(colored_leaves), # calculate proportion within each bin\n            doy_mid = mean(doy) # find mean doy of each bin for labeling\n  )\n\n# Create line plot\nggplot(autumn_binned, aes(x = doy_mid, y = prop_colored)) +\n  geom_line(color = \"#C41E3A\", size = 1) +\n  geom_point(color = \"#C41E3A\", size = 2) +\n  labs(x = \"Day of Year\",\n       y = \"Proportion with Colored Leaves\",\n       title = \"Proportion of Colored Leaves by Day of Year (2-week bins)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nFigure 3: Visualization of proportion of colored leaves in each 2-week period of the year.\n\n\n\n\nThere seems to be an increase in the proportion of colored leaves from approximately day 100 to a peak at day 300, before falling back to 0 around day 340. This reflects a seasonal trend, where leaves are increasingly more likely to be colored as fall approaches and peaks around day 300, and then fall from the trees for winter and return the proportion of colored leaves back to 0.\nLastly, I want to visualize how many observations generally lie in each year.\n\n\nCode\n# Filter data to a few years to visualize this over time\nautumn_data_decades &lt;- autumn_data %&gt;% \n  filter(year %in% c(1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010, 2020))\n\n# Create scatter plot\nggplot(autumn_data_decades, aes(x = doy, y = colored_leaves, color = year)) +\n  geom_jitter(alpha = 0.6, width = 0, height = 0.1) +\n  scale_color_gradientn(colors = c(\"#FFBF00\", \"orange\",\"#CC5500\", \"#C41E3A\", \"#660033\"), guide = \"none\") +\n  labs(x = \"Day of Year\",\n       y = \"Colored Leaves\") +\n  facet_wrap(~year) +\n  theme_minimal() \n\n\n\n\n\nFigure 4: Exploration of the effects of day of the year on the presence of colored leaves (0/1), separated by year.\n\n\n\n\nThis plot shows that there are relatively few colored leaf data points in each year of observation, which might create a challenge when trying to draw conclusions.\n\n\n\nAfter exploring the data, I want to fit a logistic model with day of year, year, and mean fall temperature as predictors and colored leaves as a binary response.\n\n# Logistic model\nbin_model &lt;- glm(colored_leaves ~ doy + year + mean_fall,\n                 data = autumn_data,\n                 family = binomial(link = \"logit\"))\n\n\n\nCode\n# Create coefficient table\ntidy(bin_model) %&gt;%\n  kable(digits = 3, caption = \"Coefficient Estimates for Logistic Model\") %&gt;% \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nCoefficient Estimates for Logistic Model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.348\n3.469\n0.389\n0.697\n\n\ndoy\n0.013\n0.001\n14.664\n0.000\n\n\nyear\n-0.003\n0.002\n-1.745\n0.081\n\n\nmean_fall\n0.029\n0.028\n1.054\n0.292\n\n\n\n\n\nOnly the day of year (doy) variable is significant, with a p-value below 0.05. The positive coefficient for doy indicates that the probability of colored leaves increases later in the year, consistent with the seasonal pattern in our data exploration. Because the other predictors were not statistically significant and there is a relatively high possibility of getting these values by chance, we cannot infer that mean fall temperature and year have a meaningful effect on the probability of colored leaves.\n\n\n\nNext, I want to look at the model fit over a selection of years to better visualize whether doy where probability of colored leaves shifts from 0 to 1 becomes earlier over time. Setting mean_fall to the average allows us to focus on the relationship between day of year and year on leaf coloration.\n\n\nCode\n# Pick four years to model\nyears &lt;- c(1900, 1940, 1980, 2020)\n\n# Make data frame of predictors using means for fixed variables\npred_grid &lt;- expand_grid(\n  doy = 1:365,\n  year = years,\n) %&gt;% \n  mutate(\n    mean_fall = mean(autumn_data$mean_fall, na.rm = TRUE)\n  )\n\n# Generate predictions using model- just best estimate\nbin_model_pred &lt;- pred_grid %&gt;% \n  mutate(p = predict(object = bin_model,\n                     newdata = pred_grid,\n                     type = \"response\"))\n\n\n\n\nCode\n# Plot predictions for three years \nggplot(bin_model_pred, aes(x = doy, y = p, color = factor(year))) +\n  geom_line() +\n  coord_cartesian(ylim = c(0,1)) +\n  scale_color_manual(values = c(\"1900\" = \"#FFBF00\", \"1940\" = \"#CC5500\", \"1980\" = \"#C41E3A\", \n                                \"2020\" = \"#660033\")) +\n  labs(color = \"Year\", y = \"p\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWe can also transition between link and response space to visualize this plot with confidence intervals.\n\n\nCode\n# Create confidence interval\nbin_model_se &lt;- predict(object = bin_model,\n                          newdata = pred_grid,\n                          type = \"link\", # stay in link space!\n                          se.fit = TRUE) # get the se of the fit\n\n# Go from link space to response space\nlinkinv &lt;- family(bin_model)$linkinv\n\nbin_model_pred_l &lt;- pred_grid %&gt;% \n  mutate(\n    # get logit(p)\n    logit_p = bin_model_se$fit,\n    # 95% CI in link space- keeps in realistic range of values!! can do math here\n    logit_p_se = bin_model_se$se.fit,\n    logit_p_lwr = qnorm(0.025, mean = logit_p, sd = logit_p_se),\n    logit_p_upr = qnorm(0.975, mean = logit_p, sd = logit_p_se),\n    # undo the link function using linkinv- can interpret here\n    p = linkinv(logit_p),\n    p_lwr = linkinv(logit_p_lwr),\n    p_upr = linkinv(logit_p_upr)\n  )\n\n# filter predictions to a few years\nbin_model_pred_l_filtered &lt;- bin_model_pred_l %&gt;%\n  filter(year %in% c(1900, 1940, 1980, 2020)) %&gt;%\n  mutate(year = factor(year))\n\n# plot \nggplot(bin_model_pred_l_filtered, aes(x = doy, y = p, fill = year))+\n  geom_ribbon(aes(ymin = p_lwr, ymax = p_upr), alpha = 0.2) +\n  scale_fill_manual(values = c(\"1900\" = \"#FFBF00\", \"1940\" = \"#CC5500\", \"1980\" = \"#C41E3A\", \n                                \"2020\" = \"#660033\")) +\n  geom_line() +\n  coord_cartesian(ylim = c(0,1)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nBased on this figure, the day of year where leaf coloration probability is 0.5 is shifting to the right from 1900 to 2020, or becoming later in the year over time.\nTo see if my results will change due to seasonality and when colored leaves are most probable, I want to make mid-summer associated with 0 as this is when there is most likely to be 0 colored leaves, and shift the other days of the year accordingly. I also want to group the data by decade to create more data points for plotting, to correct for the issues seen in Figure 3 of my data exploration.\n\n# Mutate data \nautumn_data_decadegroups &lt;- autumn_data %&gt;% \n  mutate(doy_summer = (doy - 180) %% 365,\n         decade = floor(year/10) * 10) \n\n\n# Fit model based on mutated data\nbin_model_summer &lt;- glm(colored_leaves ~ doy_summer + decade + mean_fall,\n                 data = autumn_data_decadegroups,\n                 family = binomial(link = \"logit\"))\n\n\n\nCode\n# Create coefficient table\ntidy(bin_model_summer) %&gt;%\n  kable(digits = 3, caption = \"Coefficient Estimates for Logistic Decade Model\") %&gt;% \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nCoefficient Estimates for Logistic Decade Model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.704\n3.266\n0.828\n0.408\n\n\ndoy_summer\n-0.003\n0.000\n-6.915\n0.000\n\n\ndecade\n-0.002\n0.002\n-1.280\n0.201\n\n\nmean_fall\n0.021\n0.026\n0.811\n0.417\n\n\n\n\n\nOnly day of year continued to be statistically significant, but in contrast to the previous model is negative. Let’s see what this looks like plotted!\n\n\nCode\n# Pick four decades to model\ndecades &lt;- c(1900, 1940, 1980, 2010)\n\n# Make data frame of predictors using means for fixed variables\npred_grid2 &lt;- expand_grid(\n  doy_summer = 1:365,\n  decade = decades,\n) %&gt;% \n  mutate(\n    mean_fall = mean(autumn_data_decadegroups$mean_fall, na.rm = TRUE)\n  )\n\n# Generate predictions using model- just best estimate\nbin_model_pred2 &lt;- pred_grid2 %&gt;% \n  mutate(p = predict(object = bin_model_summer,\n                     newdata = pred_grid2,\n                     type = \"response\"))\n\n\n\n\nCode\n# Plot predictions for four decades\nggplot(bin_model_pred2, aes(x = doy_summer, y = p, color = factor(decade))) +\n  geom_line() +\n  coord_cartesian(ylim = c(0,1)) +\n  labs(color = \"Decade\", y = \"p\") +\n  scale_color_manual(values = c(\"1900\" = \"#FFBF00\", \"1940\" = \"#CC5500\", \"1980\" = \"#C41E3A\", \n                                \"2020\" = \"#660033\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis plot shows a negative trend in leave coloration probability over time with a starting point of mid summer, which is unexpected given our hypothesis. The coefficient for doy_summer was slightly negative and significant, indicating that for each day after mid-summer the probability of colored leaves decreased, as shown in our plot. This might be due to a lack of data in later summer and early fall days, or collection biases where trees that showed earlier color were collected because they were more noticeable.\n\n\n\nNext, I want to apply a slightly more complex segmented model to my data. We can use a simplified model to simulate data from and better understand how a segmented model works:\n\\[\n\\text{BinaryOutcome} \\sim \\text{Binomial}(1, p)\n\\] \\[\n\\text{logit}(p) =\n\\beta_0 + \\beta_1 \\cdot \\text{x} + \\beta_2 \\cdot \\text{x} \\cdot \\text{afterbreakpoint}\n\\] Where afterbreakpoint is a binary variable where 0 corresponds to before the determined breakpoint and 1 is after. This allows us to have the different slopes depending upon where we are along the x-axis. This difference in slopes is beta 2.\nWe will use these steps:\n\nChoose a set of parameters and predictor variable(s) for the simulation\n\n\n\nCode\nset.seed(123)\n\n# 1. Choose a set of parameters \nbeta0 &lt;- 0\nbeta1 &lt;- 0.4\nbeta2 &lt;- 0.5\n\n# Create x values\nx_before &lt;- seq(0, 20, length.out = 1000)\n\n# Want breakpoint to be the middle @ x = 10\n# Subtract 10 from every x value\nx &lt;- x_before - 10\n\n# Create breakpoint indicator- now @ x = 0 \n# make this 0 if x is negative and 1 if x is positive\nafterbreakpoint &lt;- case_when(\n  x &lt; 0 ~ 0,\n  x &gt;= 0 ~ 1\n)\n\n\n\nUse a random variable to generate a response, based on the parameters and predictor(s)\n\n\n\nCode\n# Write out formula based on model notation\nlogit_p &lt;- beta0 + beta1*x + beta2*(x*afterbreakpoint)\n\np &lt;- exp(logit_p) / (1 + exp(logit_p))\n\n# 2. Use a random variable to generate Binary outcomes\ny &lt;- rbinom(n = length(p), size = 1, prob = p)\n\n# Create full simulated data\nsim_dat &lt;- tibble(x, afterbreakpoint, y) \n\n\n\nFit a model to the simulated data\n\n\n\nCode\n# 3. Fit model to simulated data\nsim_mod &lt;- glm(y ~ x + x:afterbreakpoint,\n                 data = sim_dat,\n                 family = binomial(link = \"logit\"))\n\n# Fit logistic model with x values before subtracting 10\nsim_mod_before &lt;- glm(y ~ x_before, data = sim_dat,\n                    family = binomial)\n\n# Fit segmented model \nseg_mod &lt;- segmented(sim_mod_before, seg.Z = ~ x_before)\n\n\n\nCheck if the model’s parameter estimates match selected parameters\n\n\n\nCode\n# 4. Check if model's parameters match \nmod_summ &lt;- summary(sim_mod)\nbeta_0_hat &lt;- coef(sim_mod)[1]\nbeta_1_hat &lt;- coef(sim_mod)[2]\nbeta_2_hat &lt;- coef(sim_mod)[3]\nbreakpoint_est &lt;- summary(seg_mod)$psi[1, \"Est.\"]\n\n# Create kable table with values for comparison\ncomparison_table &lt;- rbind(\n  Selected = round(c(beta0, beta1, beta2, 10), 2),\n  Model = round(c(beta_0_hat, beta_1_hat, beta_2_hat, breakpoint_est), 2))\n\ncolnames(comparison_table) &lt;- c(\"Beta 0\", \"Beta 1\", \"Beta 2\", \"Breakpoint\")\n\nkable(comparison_table, caption = \"Coefficient Comparisons\", align = \"c\") %&gt;% \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nCoefficient Comparisons\n\n\n\nBeta 0\nBeta 1\nBeta 2\nBreakpoint\n\n\n\n\nSelected\n0.00\n0.40\n0.5\n10.00\n\n\nModel\n-0.02\n0.42\n0.4\n11.43\n\n\n\n\n\nThe Intercept (beta 0) was very close to the selected value of 0, and the x (beta 1) was very close to the selected value of 0.4. The x associated with being after the breakpoint (beta 2) was slightly lower than the selected value of 0.5. The model estimated a breakpoint of 11.43, which was similar to the selected value of 10.\nNow that we know how a segmented model works, I want to apply this model to my own data to look at the rate of change between the year curves and determine whether the doy at which the probability of having colored leaves increases shifts back at a faster rate after a certain year break point. To do this, we can describe our model in statistical notation:\n\\[\n\\text{BinaryOutcome} \\sim \\text{Binomial}(1, p)\n\\]\n\\[\n\\text{logit}(p) =\n\\beta_0 + \\beta_1 \\cdot \\text{doy} + \\beta_2 \\cdot \\text{year} + \\beta_3 \\cdot \\text{year} \\cdot \\text{after} + \\beta_4 \\cdot \\text{meanfall}\n\\]\n\\[\n\\text{after}=\n\\begin{cases}\n0 & \\text{if year} \\le \\psi \\\\\n1 & \\text{if year} &gt; \\psi\n\\end{cases}\n\\]\nA segmented model allows for a change in the relationship between response and explanatory variables at a breakpoint, where beta 3 gets added to the slope of beta 2 if the year is beyond the breakpoint (Kaizer n.d.). Based on this idea, I set up my hypotheses:\n\n\n\nNull hypothesis (H0): One slope describes the effect of year on leaf coloration (slopes are the same before and after breakpoint)\n\nH0: \\(\\beta_3 = 0\\)\n\nAlternative hypothesis (Ha): Multiple slopes describe the effect of year on leaf coloration (slopes are different before and after breakpoint)\n\nHa: \\(\\beta_3 \\neq 0\\)\n\n\n\n\nNow, I want to apply a segmented model to my original logistic model at an estimated break point in 1980.\n\n# Segmented model using initial logistic model\nseg_model &lt;- segmented(bin_model, seg.Z = ~year, psi = list(year = 1980))\n\n\n\nCode\n# Create coefficient table\ntidy(seg_model) %&gt;%\n  kable(digits = 3, caption = \"Coefficient Estimates for Segmented Model\") %&gt;% \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nCoefficient Estimates for Segmented Model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-9.523\n7.893\n-1.207\n0.228\n\n\ndoy\n0.013\n0.001\n14.603\n0.000\n\n\nyear\n0.003\n0.004\n0.616\n0.538\n\n\nmean_fall\n0.027\n0.028\n0.978\n0.328\n\n\nU1.year\n-0.015\n0.007\n-2.040\n0.041\n\n\npsi1.year\n0.000\n14.130\n0.000\n1.000\n\n\n\n\n\n\n\nCode\nseg_ci &lt;- confint(seg_model)\n\n# Convert to data frame for kable\nci_table &lt;- as.data.frame(seg_ci)\nci_table$Term &lt;- rownames(ci_table)\ncolnames(ci_table) &lt;- c(\"Est.\", \"Lower\", \"Upper\")  \n\n# Reorder columns \nci_table &lt;- ci_table[, c(\"Est.\", \"Lower\", \"Upper\")]\n\n# Round and make table\nci_table %&gt;%\n  round(2) %&gt;%\n  kable(caption = \"Segmented Model 95% Confidence Intervals\", align = \"c\") %&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE)\n\n\n\nSegmented Model 95% Confidence Intervals\n\n\n\nEst.\nLower\nUpper\n\n\n\n\npsi1.year\n1968\n1940.29\n1995.71\n\n\n\n\n\nThe estimated break point (where the slope of year changes) was in 1968, with a standard error of about 14 years ranging between 1940 and 1996 (the 95% CI). This is quite a large range! The coefficients for doy, year, and mean_fall were all slightly positive, but only doy was significant, indicating that the the probability of colored leaves increases later in the year. The estimate for U1.year, or the change in slope after the breakpoint, was -0.0145. Subtracting this value from the original slope for year (beta 2, 0.0025), the estimated slope after the breakpoint was -0.012. In context, the slope for year changed from slightly positive to slightly negative after the breakpoint in 1968.\n\n\nCode\n# Make data frame of predictors using means for fixed variables\npred_grid &lt;- expand_grid(\n  doy = 1:365,\n  year = years\n) %&gt;% \n  mutate(\n    mean_fall = mean(autumn_data$mean_fall, na.rm = TRUE),\n    annual_precip = mean(autumn_data$annual_precip, na.rm = TRUE)\n  )\n\n# Generate predictions using model- just best estimate\nseg_model_pred &lt;- pred_grid %&gt;% \n  mutate(p = predict(object = seg_model,\n                     newdata = pred_grid,\n                     type = \"response\"))\n\n\n\n\nCode\n# Plot predictions for three years \nggplot(seg_model_pred, aes(x = doy, y = p, color = factor(year))) +\n  geom_line() +\n  coord_cartesian(ylim = c(0,1)) +\n  scale_color_manual(values = c(\"1900\" = \"#FFBF00\", \"1940\" = \"#CC5500\", \"1980\" = \"#C41E3A\", \n                                \"2020\" = \"#660033\")) +\n  labs(color = \"Year\", y = \"p\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n2020 has a clearly lower slope than the other years, indicating the probability of colored leaves being lower further into the year than in previous decades. We can also visualize this with confidence intervals.\n\n\nCode\n# Create confidence interval\nseg_model_se &lt;- predict(object = seg_model,\n                          newdata = pred_grid,\n                          type = \"link\",\n                          se.fit = TRUE)\n\n# Go from link space to response space\nlinkinv &lt;- family(seg_model)$linkinv\n\nseg_model_pred &lt;- pred_grid %&gt;% \n  mutate(\n    # get logit(p)\n    logit_p = seg_model_se$fit,\n    # 95% CI in link space\n    logit_p_se = seg_model_se$se.fit,\n    logit_p_lwr = qnorm(0.025, mean = logit_p, sd = logit_p_se),\n    logit_p_upr = qnorm(0.975, mean = logit_p, sd = logit_p_se),\n    # undo the link function using linkinv\n    p = linkinv(logit_p),\n    p_lwr = linkinv(logit_p_lwr),\n    p_upr = linkinv(logit_p_upr)\n  )\n\n# filter predictions to a few years\nseg_model_pred_filtered &lt;- seg_model_pred %&gt;%\n  filter(year %in% c(1900, 1940, 1980, 2020)) %&gt;%\n  mutate(year = factor(year))\n\n\n\n\nCode\n# plot \nggplot(seg_model_pred_filtered, aes(x = doy, y = p, fill = year))+\n  geom_ribbon(aes(ymin = p_lwr, ymax = p_upr), alpha = 0.2) +\n  scale_fill_manual(values = c(\"1900\" = \"#FFBF00\", \"1940\" = \"#CC5500\", \"1980\" = \"#C41E3A\", \n                                \"2020\" = \"#660033\")) +\n  geom_line() +\n  coord_cartesian(ylim = c(0,1)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nBased upon this figure, the CI for the year 2020 is visually distinct from the previous decades that were plotted.\nI also found Davies’ test for a change in slope, which is specifically for hypothesis testing for breakpoints. It evaluates whether the difference in slopes is significantly different from 0, or whether we actually need a breakpoint (Muggeo 2025).\n\n\nCode\ndavies &lt;- davies.test(bin_model, seg.Z = ~ year, k=10)\n\n# Make coefficient table \ntidy(davies) %&gt;%\n  kable(digits = 3, caption = \"Davies Test Results\") %&gt;% \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nDavies Test Results\n\n\nstatistic\np.value\nparameter\nmethod\nalternative\n\n\n\n\n1962.111\n0.346\n8\nDavies' test for a change in the slope\ntwo.sided\n\n\n\n\n\n\n\n\nIf we look back at our hypotheses:\n\nNull hypothesis (H0): One slope describes the effect of year on leaf coloration (slopes are the same before and after breakpoint)\n\nH0: \\(\\beta_3 = 0\\)\n\nAlternative hypothesis (Ha): Multiple slopes describe the effect of year on leaf coloration (slopes are different before and after breakpoint)\n\nHa: \\(\\beta_3 \\neq 0\\)\nBecause the resulting p-value was 0.35 and far above 0.05, we cannot reject H0 that one slope describes the effect of year on leaf coloration, and therefore don’t see that there is a significant change in the shift in the doy of leave coloration over time. A lot of our results were not intuitive, as we would assume that leaf coloration would be seen earlier in the year due to climate change and warmer temperatures being experienced earlier in the year. The difference in our results might be due to different statistical methodology or an uneven sample, where there were far less specimens with colored leaves to include as data in our model. In further analysis, I would like to dive deeper into what variables might be confounding these results such as seasonality or location, or attempt to use a GAM to look at non-linear relationships between predictor and response variables. Lastly, the data set contained a binary variable for leaf presence, which could affect our results as Maple leaves fall after changing color and would create a narrow window to detect color."
  },
  {
    "objectID": "posts/2025-12-01-eds222/index.html#question-and-data",
    "href": "posts/2025-12-01-eds222/index.html#question-and-data",
    "title": "Herbarium Specimen Phenology",
    "section": "",
    "text": "Many studies have found climate-related delays in the timing of leaf coloration, but do not have a lot of data from before the late 20th century (Garretson and Forkner 2021b).\nA data set collected by Garretson and Forkner in 2021 from the Southeast Regional Network of Expertise and Collections (SERNEC n.d.) contains evaluations of 2,972 digitized herbaria specimens of red and sugar maples collected between 1826 and 2016. They paired these specimens with climate and locality information to investigate long-term trends in autumn phenology over the past century (Garretson and Forkner 2021a).\nUsing this data, I want to analyze the effects of day of year (doy), year, and mean fall temperature on the presence of colored leaves as well as determine whether the day of the year when there becomes a higher probability of leaf color is shifting earlier over time.\n\n\n\n\nDAG\n\n\n\nI think that year, day of the year, and mean fall temperature all have an effect on whether a leaf is colored. Day of year is a seasonal relationship, where leaf coloration is expected the least in the summer (~180-250 doy) and more after that period in the fall (~250-300 doy). Cooler fall temperatures could cause a quicker onset of leaf coloration and senescence, while warmer mean temperatures could delay it. Year could affect leaf coloration through long-term trends in climate, and could also directly affect mean fall temperatures, as some years are generally warmer or cooler than others.\n\n\n\n\nCode\nlibrary(here)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(segmented)\nlibrary(kableExtra)\nlibrary(broom)\nlibrary(gtsummary)\nlibrary(modelsummary)\n\n# Read in data\nautumn_data &lt;- read_csv(here::here(\"posts\", \"2025-12-01-eds222\", \"data\", \"autumn_data.csv\")) %&gt;% \n  clean_names() \n\n\n\n\n\n\n\nCode\nggplot(autumn_data, aes(x = longitude, y = latitude)) +\n  geom_point(color = \"darkgreen\") +\n  labs(x = \"Longitude\", y = \"Latitude\", \n       title = \"Herbarium Specimen Locations\") +\n  theme_minimal()\n\n\n\n\n\nFigure 1: Initial exploration of the spatial distribution of Herbarium specimens.\n\n\n\n\nThe specimens are located relatively uniformly throughout the Northeast.\n\n\nCode\n# Create scatter plot\nggplot(autumn_data, aes(x = doy, y = colored_leaves, color = species)) +\n  geom_jitter(alpha = 0.5, width = 0, height = 0.1) +\n  scale_color_manual(values = c(\"#6F8FAF\", \"#8A9A5B\")) +\n  labs(x = \"Day of Year\",\n       y = \"Colored Leaves\") +\n  theme_minimal() \n\n\n\n\n\nFigure 2: Initial exploration of the effects of day of the year on the presence of colored leaves (0/1).\n\n\n\n\nThe binary colored_leaves variable appears to have many more specimens with non-colored leaves, which are spread throughout the entire year but show a cluster between about doy 80 and 300. There are fewer specimens with colored leaves, with a more concentrated spread between doy 100 and 320. This data distribution suggests that a logistic regression model could be a good fit.\nA better way to visualize the colored_leaves variable and potentially see a seasonal trend is to group our observations into 2-week bins and calculate the proportion of colored leaves within each bin.\n\n\nCode\n# Create 14-day bins\nautumn_binned &lt;- autumn_data %&gt;%\n  mutate(doy_bin = cut(doy, breaks = seq(0, 366, by = 14), include.lowest = TRUE, right = FALSE)) %&gt;%\n  group_by(doy_bin) %&gt;%\n  summarize(n = n(),  # total number within each bin\n            prop_colored = mean(colored_leaves), # calculate proportion within each bin\n            doy_mid = mean(doy) # find mean doy of each bin for labeling\n  )\n\n# Create line plot\nggplot(autumn_binned, aes(x = doy_mid, y = prop_colored)) +\n  geom_line(color = \"#C41E3A\", size = 1) +\n  geom_point(color = \"#C41E3A\", size = 2) +\n  labs(x = \"Day of Year\",\n       y = \"Proportion with Colored Leaves\",\n       title = \"Proportion of Colored Leaves by Day of Year (2-week bins)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nFigure 3: Visualization of proportion of colored leaves in each 2-week period of the year.\n\n\n\n\nThere seems to be an increase in the proportion of colored leaves from approximately day 100 to a peak at day 300, before falling back to 0 around day 340. This reflects a seasonal trend, where leaves are increasingly more likely to be colored as fall approaches and peaks around day 300, and then fall from the trees for winter and return the proportion of colored leaves back to 0.\nLastly, I want to visualize how many observations generally lie in each year.\n\n\nCode\n# Filter data to a few years to visualize this over time\nautumn_data_decades &lt;- autumn_data %&gt;% \n  filter(year %in% c(1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010, 2020))\n\n# Create scatter plot\nggplot(autumn_data_decades, aes(x = doy, y = colored_leaves, color = year)) +\n  geom_jitter(alpha = 0.6, width = 0, height = 0.1) +\n  scale_color_gradientn(colors = c(\"#FFBF00\", \"orange\",\"#CC5500\", \"#C41E3A\", \"#660033\"), guide = \"none\") +\n  labs(x = \"Day of Year\",\n       y = \"Colored Leaves\") +\n  facet_wrap(~year) +\n  theme_minimal() \n\n\n\n\n\nFigure 4: Exploration of the effects of day of the year on the presence of colored leaves (0/1), separated by year.\n\n\n\n\nThis plot shows that there are relatively few colored leaf data points in each year of observation, which might create a challenge when trying to draw conclusions.\n\n\n\nAfter exploring the data, I want to fit a logistic model with day of year, year, and mean fall temperature as predictors and colored leaves as a binary response.\n\n# Logistic model\nbin_model &lt;- glm(colored_leaves ~ doy + year + mean_fall,\n                 data = autumn_data,\n                 family = binomial(link = \"logit\"))\n\n\n\nCode\n# Create coefficient table\ntidy(bin_model) %&gt;%\n  kable(digits = 3, caption = \"Coefficient Estimates for Logistic Model\") %&gt;% \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nCoefficient Estimates for Logistic Model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.348\n3.469\n0.389\n0.697\n\n\ndoy\n0.013\n0.001\n14.664\n0.000\n\n\nyear\n-0.003\n0.002\n-1.745\n0.081\n\n\nmean_fall\n0.029\n0.028\n1.054\n0.292\n\n\n\n\n\nOnly the day of year (doy) variable is significant, with a p-value below 0.05. The positive coefficient for doy indicates that the probability of colored leaves increases later in the year, consistent with the seasonal pattern in our data exploration. Because the other predictors were not statistically significant and there is a relatively high possibility of getting these values by chance, we cannot infer that mean fall temperature and year have a meaningful effect on the probability of colored leaves.\n\n\n\nNext, I want to look at the model fit over a selection of years to better visualize whether doy where probability of colored leaves shifts from 0 to 1 becomes earlier over time. Setting mean_fall to the average allows us to focus on the relationship between day of year and year on leaf coloration.\n\n\nCode\n# Pick four years to model\nyears &lt;- c(1900, 1940, 1980, 2020)\n\n# Make data frame of predictors using means for fixed variables\npred_grid &lt;- expand_grid(\n  doy = 1:365,\n  year = years,\n) %&gt;% \n  mutate(\n    mean_fall = mean(autumn_data$mean_fall, na.rm = TRUE)\n  )\n\n# Generate predictions using model- just best estimate\nbin_model_pred &lt;- pred_grid %&gt;% \n  mutate(p = predict(object = bin_model,\n                     newdata = pred_grid,\n                     type = \"response\"))\n\n\n\n\nCode\n# Plot predictions for three years \nggplot(bin_model_pred, aes(x = doy, y = p, color = factor(year))) +\n  geom_line() +\n  coord_cartesian(ylim = c(0,1)) +\n  scale_color_manual(values = c(\"1900\" = \"#FFBF00\", \"1940\" = \"#CC5500\", \"1980\" = \"#C41E3A\", \n                                \"2020\" = \"#660033\")) +\n  labs(color = \"Year\", y = \"p\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWe can also transition between link and response space to visualize this plot with confidence intervals.\n\n\nCode\n# Create confidence interval\nbin_model_se &lt;- predict(object = bin_model,\n                          newdata = pred_grid,\n                          type = \"link\", # stay in link space!\n                          se.fit = TRUE) # get the se of the fit\n\n# Go from link space to response space\nlinkinv &lt;- family(bin_model)$linkinv\n\nbin_model_pred_l &lt;- pred_grid %&gt;% \n  mutate(\n    # get logit(p)\n    logit_p = bin_model_se$fit,\n    # 95% CI in link space- keeps in realistic range of values!! can do math here\n    logit_p_se = bin_model_se$se.fit,\n    logit_p_lwr = qnorm(0.025, mean = logit_p, sd = logit_p_se),\n    logit_p_upr = qnorm(0.975, mean = logit_p, sd = logit_p_se),\n    # undo the link function using linkinv- can interpret here\n    p = linkinv(logit_p),\n    p_lwr = linkinv(logit_p_lwr),\n    p_upr = linkinv(logit_p_upr)\n  )\n\n# filter predictions to a few years\nbin_model_pred_l_filtered &lt;- bin_model_pred_l %&gt;%\n  filter(year %in% c(1900, 1940, 1980, 2020)) %&gt;%\n  mutate(year = factor(year))\n\n# plot \nggplot(bin_model_pred_l_filtered, aes(x = doy, y = p, fill = year))+\n  geom_ribbon(aes(ymin = p_lwr, ymax = p_upr), alpha = 0.2) +\n  scale_fill_manual(values = c(\"1900\" = \"#FFBF00\", \"1940\" = \"#CC5500\", \"1980\" = \"#C41E3A\", \n                                \"2020\" = \"#660033\")) +\n  geom_line() +\n  coord_cartesian(ylim = c(0,1)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nBased on this figure, the day of year where leaf coloration probability is 0.5 is shifting to the right from 1900 to 2020, or becoming later in the year over time.\nTo see if my results will change due to seasonality and when colored leaves are most probable, I want to make mid-summer associated with 0 as this is when there is most likely to be 0 colored leaves, and shift the other days of the year accordingly. I also want to group the data by decade to create more data points for plotting, to correct for the issues seen in Figure 3 of my data exploration.\n\n# Mutate data \nautumn_data_decadegroups &lt;- autumn_data %&gt;% \n  mutate(doy_summer = (doy - 180) %% 365,\n         decade = floor(year/10) * 10) \n\n\n# Fit model based on mutated data\nbin_model_summer &lt;- glm(colored_leaves ~ doy_summer + decade + mean_fall,\n                 data = autumn_data_decadegroups,\n                 family = binomial(link = \"logit\"))\n\n\n\nCode\n# Create coefficient table\ntidy(bin_model_summer) %&gt;%\n  kable(digits = 3, caption = \"Coefficient Estimates for Logistic Decade Model\") %&gt;% \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nCoefficient Estimates for Logistic Decade Model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.704\n3.266\n0.828\n0.408\n\n\ndoy_summer\n-0.003\n0.000\n-6.915\n0.000\n\n\ndecade\n-0.002\n0.002\n-1.280\n0.201\n\n\nmean_fall\n0.021\n0.026\n0.811\n0.417\n\n\n\n\n\nOnly day of year continued to be statistically significant, but in contrast to the previous model is negative. Let’s see what this looks like plotted!\n\n\nCode\n# Pick four decades to model\ndecades &lt;- c(1900, 1940, 1980, 2010)\n\n# Make data frame of predictors using means for fixed variables\npred_grid2 &lt;- expand_grid(\n  doy_summer = 1:365,\n  decade = decades,\n) %&gt;% \n  mutate(\n    mean_fall = mean(autumn_data_decadegroups$mean_fall, na.rm = TRUE)\n  )\n\n# Generate predictions using model- just best estimate\nbin_model_pred2 &lt;- pred_grid2 %&gt;% \n  mutate(p = predict(object = bin_model_summer,\n                     newdata = pred_grid2,\n                     type = \"response\"))\n\n\n\n\nCode\n# Plot predictions for four decades\nggplot(bin_model_pred2, aes(x = doy_summer, y = p, color = factor(decade))) +\n  geom_line() +\n  coord_cartesian(ylim = c(0,1)) +\n  labs(color = \"Decade\", y = \"p\") +\n  scale_color_manual(values = c(\"1900\" = \"#FFBF00\", \"1940\" = \"#CC5500\", \"1980\" = \"#C41E3A\", \n                                \"2020\" = \"#660033\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis plot shows a negative trend in leave coloration probability over time with a starting point of mid summer, which is unexpected given our hypothesis. The coefficient for doy_summer was slightly negative and significant, indicating that for each day after mid-summer the probability of colored leaves decreased, as shown in our plot. This might be due to a lack of data in later summer and early fall days, or collection biases where trees that showed earlier color were collected because they were more noticeable.\n\n\n\nNext, I want to apply a slightly more complex segmented model to my data. We can use a simplified model to simulate data from and better understand how a segmented model works:\n\\[\n\\text{BinaryOutcome} \\sim \\text{Binomial}(1, p)\n\\] \\[\n\\text{logit}(p) =\n\\beta_0 + \\beta_1 \\cdot \\text{x} + \\beta_2 \\cdot \\text{x} \\cdot \\text{afterbreakpoint}\n\\] Where afterbreakpoint is a binary variable where 0 corresponds to before the determined breakpoint and 1 is after. This allows us to have the different slopes depending upon where we are along the x-axis. This difference in slopes is beta 2.\nWe will use these steps:\n\nChoose a set of parameters and predictor variable(s) for the simulation\n\n\n\nCode\nset.seed(123)\n\n# 1. Choose a set of parameters \nbeta0 &lt;- 0\nbeta1 &lt;- 0.4\nbeta2 &lt;- 0.5\n\n# Create x values\nx_before &lt;- seq(0, 20, length.out = 1000)\n\n# Want breakpoint to be the middle @ x = 10\n# Subtract 10 from every x value\nx &lt;- x_before - 10\n\n# Create breakpoint indicator- now @ x = 0 \n# make this 0 if x is negative and 1 if x is positive\nafterbreakpoint &lt;- case_when(\n  x &lt; 0 ~ 0,\n  x &gt;= 0 ~ 1\n)\n\n\n\nUse a random variable to generate a response, based on the parameters and predictor(s)\n\n\n\nCode\n# Write out formula based on model notation\nlogit_p &lt;- beta0 + beta1*x + beta2*(x*afterbreakpoint)\n\np &lt;- exp(logit_p) / (1 + exp(logit_p))\n\n# 2. Use a random variable to generate Binary outcomes\ny &lt;- rbinom(n = length(p), size = 1, prob = p)\n\n# Create full simulated data\nsim_dat &lt;- tibble(x, afterbreakpoint, y) \n\n\n\nFit a model to the simulated data\n\n\n\nCode\n# 3. Fit model to simulated data\nsim_mod &lt;- glm(y ~ x + x:afterbreakpoint,\n                 data = sim_dat,\n                 family = binomial(link = \"logit\"))\n\n# Fit logistic model with x values before subtracting 10\nsim_mod_before &lt;- glm(y ~ x_before, data = sim_dat,\n                    family = binomial)\n\n# Fit segmented model \nseg_mod &lt;- segmented(sim_mod_before, seg.Z = ~ x_before)\n\n\n\nCheck if the model’s parameter estimates match selected parameters\n\n\n\nCode\n# 4. Check if model's parameters match \nmod_summ &lt;- summary(sim_mod)\nbeta_0_hat &lt;- coef(sim_mod)[1]\nbeta_1_hat &lt;- coef(sim_mod)[2]\nbeta_2_hat &lt;- coef(sim_mod)[3]\nbreakpoint_est &lt;- summary(seg_mod)$psi[1, \"Est.\"]\n\n# Create kable table with values for comparison\ncomparison_table &lt;- rbind(\n  Selected = round(c(beta0, beta1, beta2, 10), 2),\n  Model = round(c(beta_0_hat, beta_1_hat, beta_2_hat, breakpoint_est), 2))\n\ncolnames(comparison_table) &lt;- c(\"Beta 0\", \"Beta 1\", \"Beta 2\", \"Breakpoint\")\n\nkable(comparison_table, caption = \"Coefficient Comparisons\", align = \"c\") %&gt;% \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nCoefficient Comparisons\n\n\n\nBeta 0\nBeta 1\nBeta 2\nBreakpoint\n\n\n\n\nSelected\n0.00\n0.40\n0.5\n10.00\n\n\nModel\n-0.02\n0.42\n0.4\n11.43\n\n\n\n\n\nThe Intercept (beta 0) was very close to the selected value of 0, and the x (beta 1) was very close to the selected value of 0.4. The x associated with being after the breakpoint (beta 2) was slightly lower than the selected value of 0.5. The model estimated a breakpoint of 11.43, which was similar to the selected value of 10.\nNow that we know how a segmented model works, I want to apply this model to my own data to look at the rate of change between the year curves and determine whether the doy at which the probability of having colored leaves increases shifts back at a faster rate after a certain year break point. To do this, we can describe our model in statistical notation:\n\\[\n\\text{BinaryOutcome} \\sim \\text{Binomial}(1, p)\n\\]\n\\[\n\\text{logit}(p) =\n\\beta_0 + \\beta_1 \\cdot \\text{doy} + \\beta_2 \\cdot \\text{year} + \\beta_3 \\cdot \\text{year} \\cdot \\text{after} + \\beta_4 \\cdot \\text{meanfall}\n\\]\n\\[\n\\text{after}=\n\\begin{cases}\n0 & \\text{if year} \\le \\psi \\\\\n1 & \\text{if year} &gt; \\psi\n\\end{cases}\n\\]\nA segmented model allows for a change in the relationship between response and explanatory variables at a breakpoint, where beta 3 gets added to the slope of beta 2 if the year is beyond the breakpoint (Kaizer n.d.). Based on this idea, I set up my hypotheses:\n\n\n\nNull hypothesis (H0): One slope describes the effect of year on leaf coloration (slopes are the same before and after breakpoint)\n\nH0: \\(\\beta_3 = 0\\)\n\nAlternative hypothesis (Ha): Multiple slopes describe the effect of year on leaf coloration (slopes are different before and after breakpoint)\n\nHa: \\(\\beta_3 \\neq 0\\)\n\n\n\n\nNow, I want to apply a segmented model to my original logistic model at an estimated break point in 1980.\n\n# Segmented model using initial logistic model\nseg_model &lt;- segmented(bin_model, seg.Z = ~year, psi = list(year = 1980))\n\n\n\nCode\n# Create coefficient table\ntidy(seg_model) %&gt;%\n  kable(digits = 3, caption = \"Coefficient Estimates for Segmented Model\") %&gt;% \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nCoefficient Estimates for Segmented Model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-9.523\n7.893\n-1.207\n0.228\n\n\ndoy\n0.013\n0.001\n14.603\n0.000\n\n\nyear\n0.003\n0.004\n0.616\n0.538\n\n\nmean_fall\n0.027\n0.028\n0.978\n0.328\n\n\nU1.year\n-0.015\n0.007\n-2.040\n0.041\n\n\npsi1.year\n0.000\n14.130\n0.000\n1.000\n\n\n\n\n\n\n\nCode\nseg_ci &lt;- confint(seg_model)\n\n# Convert to data frame for kable\nci_table &lt;- as.data.frame(seg_ci)\nci_table$Term &lt;- rownames(ci_table)\ncolnames(ci_table) &lt;- c(\"Est.\", \"Lower\", \"Upper\")  \n\n# Reorder columns \nci_table &lt;- ci_table[, c(\"Est.\", \"Lower\", \"Upper\")]\n\n# Round and make table\nci_table %&gt;%\n  round(2) %&gt;%\n  kable(caption = \"Segmented Model 95% Confidence Intervals\", align = \"c\") %&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE)\n\n\n\nSegmented Model 95% Confidence Intervals\n\n\n\nEst.\nLower\nUpper\n\n\n\n\npsi1.year\n1968\n1940.29\n1995.71\n\n\n\n\n\nThe estimated break point (where the slope of year changes) was in 1968, with a standard error of about 14 years ranging between 1940 and 1996 (the 95% CI). This is quite a large range! The coefficients for doy, year, and mean_fall were all slightly positive, but only doy was significant, indicating that the the probability of colored leaves increases later in the year. The estimate for U1.year, or the change in slope after the breakpoint, was -0.0145. Subtracting this value from the original slope for year (beta 2, 0.0025), the estimated slope after the breakpoint was -0.012. In context, the slope for year changed from slightly positive to slightly negative after the breakpoint in 1968.\n\n\nCode\n# Make data frame of predictors using means for fixed variables\npred_grid &lt;- expand_grid(\n  doy = 1:365,\n  year = years\n) %&gt;% \n  mutate(\n    mean_fall = mean(autumn_data$mean_fall, na.rm = TRUE),\n    annual_precip = mean(autumn_data$annual_precip, na.rm = TRUE)\n  )\n\n# Generate predictions using model- just best estimate\nseg_model_pred &lt;- pred_grid %&gt;% \n  mutate(p = predict(object = seg_model,\n                     newdata = pred_grid,\n                     type = \"response\"))\n\n\n\n\nCode\n# Plot predictions for three years \nggplot(seg_model_pred, aes(x = doy, y = p, color = factor(year))) +\n  geom_line() +\n  coord_cartesian(ylim = c(0,1)) +\n  scale_color_manual(values = c(\"1900\" = \"#FFBF00\", \"1940\" = \"#CC5500\", \"1980\" = \"#C41E3A\", \n                                \"2020\" = \"#660033\")) +\n  labs(color = \"Year\", y = \"p\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n2020 has a clearly lower slope than the other years, indicating the probability of colored leaves being lower further into the year than in previous decades. We can also visualize this with confidence intervals.\n\n\nCode\n# Create confidence interval\nseg_model_se &lt;- predict(object = seg_model,\n                          newdata = pred_grid,\n                          type = \"link\",\n                          se.fit = TRUE)\n\n# Go from link space to response space\nlinkinv &lt;- family(seg_model)$linkinv\n\nseg_model_pred &lt;- pred_grid %&gt;% \n  mutate(\n    # get logit(p)\n    logit_p = seg_model_se$fit,\n    # 95% CI in link space\n    logit_p_se = seg_model_se$se.fit,\n    logit_p_lwr = qnorm(0.025, mean = logit_p, sd = logit_p_se),\n    logit_p_upr = qnorm(0.975, mean = logit_p, sd = logit_p_se),\n    # undo the link function using linkinv\n    p = linkinv(logit_p),\n    p_lwr = linkinv(logit_p_lwr),\n    p_upr = linkinv(logit_p_upr)\n  )\n\n# filter predictions to a few years\nseg_model_pred_filtered &lt;- seg_model_pred %&gt;%\n  filter(year %in% c(1900, 1940, 1980, 2020)) %&gt;%\n  mutate(year = factor(year))\n\n\n\n\nCode\n# plot \nggplot(seg_model_pred_filtered, aes(x = doy, y = p, fill = year))+\n  geom_ribbon(aes(ymin = p_lwr, ymax = p_upr), alpha = 0.2) +\n  scale_fill_manual(values = c(\"1900\" = \"#FFBF00\", \"1940\" = \"#CC5500\", \"1980\" = \"#C41E3A\", \n                                \"2020\" = \"#660033\")) +\n  geom_line() +\n  coord_cartesian(ylim = c(0,1)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nBased upon this figure, the CI for the year 2020 is visually distinct from the previous decades that were plotted.\nI also found Davies’ test for a change in slope, which is specifically for hypothesis testing for breakpoints. It evaluates whether the difference in slopes is significantly different from 0, or whether we actually need a breakpoint (Muggeo 2025).\n\n\nCode\ndavies &lt;- davies.test(bin_model, seg.Z = ~ year, k=10)\n\n# Make coefficient table \ntidy(davies) %&gt;%\n  kable(digits = 3, caption = \"Davies Test Results\") %&gt;% \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nDavies Test Results\n\n\nstatistic\np.value\nparameter\nmethod\nalternative\n\n\n\n\n1962.111\n0.346\n8\nDavies' test for a change in the slope\ntwo.sided\n\n\n\n\n\n\n\n\nIf we look back at our hypotheses:\n\nNull hypothesis (H0): One slope describes the effect of year on leaf coloration (slopes are the same before and after breakpoint)\n\nH0: \\(\\beta_3 = 0\\)\n\nAlternative hypothesis (Ha): Multiple slopes describe the effect of year on leaf coloration (slopes are different before and after breakpoint)\n\nHa: \\(\\beta_3 \\neq 0\\)\nBecause the resulting p-value was 0.35 and far above 0.05, we cannot reject H0 that one slope describes the effect of year on leaf coloration, and therefore don’t see that there is a significant change in the shift in the doy of leave coloration over time. A lot of our results were not intuitive, as we would assume that leaf coloration would be seen earlier in the year due to climate change and warmer temperatures being experienced earlier in the year. The difference in our results might be due to different statistical methodology or an uneven sample, where there were far less specimens with colored leaves to include as data in our model. In further analysis, I would like to dive deeper into what variables might be confounding these results such as seasonality or location, or attempt to use a GAM to look at non-linear relationships between predictor and response variables. Lastly, the data set contained a binary variable for leaf presence, which could affect our results as Maple leaves fall after changing color and would create a narrow window to detect color."
  },
  {
    "objectID": "posts/2025-12-01-eds220/index.html",
    "href": "posts/2025-12-01-eds220/index.html",
    "title": "Los Angeles Wildfires: False Color Imagery & Socioeconomic Analysis",
    "section": "",
    "text": "The Palisades and Eaton Fires occurred in early January 2025 in very populated areas of Los Angeles County, California. Both fires began on January 7th and burned a combined total of about 37,469 acres (Starr and Morton 2025).\nIn this blog, we will walk through how to create a false color image using Landsat remote sensing and fire perimeter data to highlight the locations of the Palisades and Eaton Fires. We will also look at socioeconomic risk factors that could influence a community’s response to a wildfire in the form of a chloropleth map.\nCheck out the full analysis in detail on the associated GitHub repository!"
  },
  {
    "objectID": "posts/2025-12-01-eds220/index.html#social-dimensions-of-eaton-and-palisades-fires",
    "href": "posts/2025-12-01-eds220/index.html#social-dimensions-of-eaton-and-palisades-fires",
    "title": "Los Angeles Wildfires: False Color Imagery & Socioeconomic Analysis",
    "section": "Social dimensions of Eaton and Palisades fires",
    "text": "Social dimensions of Eaton and Palisades fires\nNext, we want to look at the distribution of a socioeconomic variable within the bounds of the fires to better understand how the fires might have impacted different groups within the communities. One of the variables included in the Environmental Justice Index is the percentage of of persons without internet within each census tract.\n\n1. EJI data exploration\nFirst, we want to import the EJI data and do some initial exploration.\n\n\nCode\n# Load EJI data\nfp = os.path.join('data', 'EJI_2024_California', 'EJI_2024_California.gdb')\neji = gpd.read_file(fp)\n\n# Check geometry type\nprint('Geometry type:', eji.geom_type.iloc[0])\n\n# Ensure the CRSs of all data sets match\neji = eji.to_crs(epsg = 3857)\npalisades = palisades_fire.to_crs(epsg = 3857)\neaton = eaton_fire.to_crs(epsg = 3857)\n\n# Confirm changes\nprint('Palisades and EJI CRS match?:', palisades.crs == eji.crs)\nprint('Eaton and EJI CRS match?:', eaton.crs == eji.crs)\n\n\nGeometry type: MultiPolygon\nPalisades and EJI CRS match?: True\nEaton and EJI CRS match?: True\n\n\nWe need to spatially join the EJI data with the fire perimeters based on the geometry column, so we use an inner join to keep only the rows from EJI that intersect with the fire perimeter polygons.\n\n\nCode\n# Spatially join EJI data with fire perimeters on geometry column\ncensus_within_palisades = gpd.sjoin(eji, palisades, how = 'inner', predicate = 'intersects')\ncensus_within_eaton = gpd.sjoin(eji, eaton, how = 'inner', predicate = 'intersects')\n\n\nOur data now contains both fire perimeter and EJI data.\n\n\nCode\n# Confirm join\ncensus_within_palisades.head(3)\n\n\n\n\n\n\n\n\n\nOBJECTID_left\nSTATEFP\nCOUNTYFP\nTRACTCE\nAFFGEOID\nGEOID\nGEOID_2020\nCOUNTY\nStateDesc\nSTATEABBR\n...\nTribe_Names\nTribe_Flag\nShape_Length\nShape_Area\ngeometry\nindex_right\nOBJECTID_right\ntype\nShape__Are\nShape__Len\n\n\n\n\n1328\n6505\n06\n037\n262706\n140000US06037262706\n06037262706\n06037262706\nLos Angeles County\nCalifornia\nCA\n...\n-999\n-999\n6486.563559\n2.147847e+06\nMULTIPOLYGON (((-13195148.412 4033844.678, -13...\n0\n1\nHeat Perimeter\n1182.082031\n267.101144\n\n\n1328\n6505\n06\n037\n262706\n140000US06037262706\n06037262706\n06037262706\nLos Angeles County\nCalifornia\nCA\n...\n-999\n-999\n6486.563559\n2.147847e+06\nMULTIPOLYGON (((-13195148.412 4033844.678, -13...\n1\n2\nHeat Perimeter\n2222.488281\n185.498783\n\n\n1328\n6505\n06\n037\n262706\n140000US06037262706\n06037262706\n06037262706\nLos Angeles County\nCalifornia\nCA\n...\n-999\n-999\n6486.563559\n2.147847e+06\nMULTIPOLYGON (((-13195148.412 4033844.678, -13...\n2\n3\nHeat Perimeter\n21.011719\n22.412814\n\n\n\n\n3 rows × 179 columns\n\n\n\nWhen plotting this joined data, we see that the census tracts that were kept due to an intersection cover far more space than the fire perimeters, shown in dark red.\n\n\nCode\n# Create exploratory map \nfig, ax = plt.subplots(figsize=(9,5), nrows = 1, ncols = 2)\n\ncensus_within_palisades.plot(ax=ax[0], color = \"lightblue\", edgecolor = \"darkblue\", linewidth = 0.5)\npalisades.boundary.plot(ax=ax[0], color = 'darkred')\ncensus_within_eaton.plot(ax=ax[1], color = \"lightblue\", edgecolor = \"darkblue\", linewidth = 0.5)\neaton.boundary.plot(ax=ax[1], color = 'darkred')\n\nfor a in ax:\n  a.axis('off')\n  \n# Add legends\nlegend_elements = [\n    Patch(facecolor = 'darkred', edgecolor = 'darkred', label = 'Fire perimeters'),\n    Patch(facecolor = 'darkblue', edgecolor = 'darkblue', label = 'Census tract boundaries')\n]\n\nax[0].legend(handles = legend_elements, loc = 'upper left', bbox_to_anchor=(-0.2, 1.05), fontsize = 8)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n3. Polygon clipping\nTo further reduce the census tracts to the fire perimeter boundaries, we will clip the census tracts to Palisades and Eaton fire perimeter using geopandas.clip().\n\n\nCode\npalisades_clipped = gpd.clip(census_within_palisades, palisades)\neaton_clipped = gpd.clip(census_within_eaton, eaton)\n\n\n\n\nCode\n# Create exploratory map \nfig, ax = plt.subplots(figsize=(9,5), nrows = 1, ncols = 2)\n\npalisades.boundary.plot(ax=ax[0], color = 'darkred')\npalisades_clipped.plot(ax=ax[0], color = \"lightblue\", edgecolor = \"darkblue\", linewidth = 0.5)\neaton.boundary.plot(ax=ax[1], color = 'darkred')\neaton_clipped.plot(ax=ax[1], color = \"lightblue\", edgecolor = \"darkblue\", linewidth = 0.5)\n\nfor a in ax:\n  a.axis('off')\n  \n# Add legends\nlegend_elements = [\n    Patch(facecolor = 'darkred', edgecolor = 'darkred', label = 'Fire perimeters'),\n    Patch(facecolor = 'darkblue', edgecolor = 'darkblue', label = 'Census tract boundaries')\n]\n\nax[0].legend(handles = legend_elements, loc = 'upper left', bbox_to_anchor=(-0.2, 1.05), fontsize = 8)\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n5. Visualize EJI data\nLastly, we can use these clipped polygons to visualize the values of our variable of interest (percentage of persons without internet) within the census tracts within the fire perimeters.\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n# Include our EJI variable\neji_variable = 'E_NOINT'\n\n# Find common min/max for legend range\nvmin = min(census_within_palisades[eji_variable].min(), census_within_eaton[eji_variable].min())\nvmax = max(census_within_palisades[eji_variable].max(), census_within_eaton[eji_variable].max())\n\n# Plot census tracts within Palisades perimeter\npalisades_clipped.plot(\n    column= eji_variable,\n    cmap = 'PuBuGn',\n    vmin=vmin, vmax=vmax,\n    legend=False,\n    ax=ax1,\n)\nax1.set_title('Palisades Fire')\nax1.axis('off')\n\n# Plot census tracts within Eaton perimeter\neaton_clipped.plot(\n    column=eji_variable,\n    cmap = 'PuBuGn',\n    vmin=vmin, vmax=vmax,\n    legend=False,\n    ax=ax2,\n)\nax2.set_title('Eaton Fire')\nax2.axis('off')\n\n# Add overall title\nfig.suptitle('Percentage of Persons Without Internet - Fire Areas Comparison')\n\n# Add shared colorbar at the bottom\nsm = plt.cm.ScalarMappable(cmap= 'PuBuGn', norm=plt.Normalize(vmin=vmin, vmax=vmax))\ncbar_ax = fig.add_axes([0.25, 0.08, 0.5, 0.02])  # [left, bottom, width, height]\ncbar = fig.colorbar(sm, cax=cbar_ax, orientation='horizontal')\ncbar.set_label('Persons without internet (%)')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nBased upon this map, we can see that some areas, especially within the Eaton Fire perimeter, had percentages of people without internet as high as 16%. Targeting these census tracts for wildfire resilience programs such as providing pre-planned courses of action when signs of fire are noticed or a list of places to notify using alternative methods in the case of a fire would be beneficial."
  },
  {
    "objectID": "posts/2025-12-01-eds223/index.html",
    "href": "posts/2025-12-01-eds223/index.html",
    "title": "Houston Power Grid Analysis",
    "section": "",
    "text": "As climate change increases the frequency of extreme weather events, the impacts associated with these events are felt more intensely by the communities that live there. Between February 13-17, 2021, Texas experienced a power crisis due to a severe winter storm named Uri. The increased demand for electricity for heating strained the Texas power grid and caused the implementation of rolling blackouts, leading to extended power outages for many (Zhou et al. 2024). The Texas power grid is managed by the Electrical Reliability Council of Texas (ERCOCT) and operates independently, not connected to other states’ power grids or able to borrow from them in a crisis (Monahan et al. 2024). As a result of this storm, at least 246 people died and $195 billion dollars worth of damage were caused (Zhou et al. 2024). By understanding how this impact of losing power was felt across the Houston community, we can allocate resources and plan more strategically for future weather events.\nObjective: Throughout this blog, we will use VIIRS, OpenStreetMap, and U.S. Census data to visualize the impacts of the winter storms by estimating the number of homes in the Houston metropolitan area that lost power and combine this with socioeconomic data about median household income to investigate whether these impacts were disproportionately felt."
  },
  {
    "objectID": "posts/2025-12-01-eds223/index.html#load-in-data",
    "href": "posts/2025-12-01-eds223/index.html#load-in-data",
    "title": "Houston Power Grid Analysis",
    "section": "1. Load in data",
    "text": "1. Load in data\n\nA) Night lights VIIRS data\nThe VIIRS data used for visualizing the extent of power outages around the day of the storm is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Two 10x10 degree tiles per date of interest were used for this analysis (“VIIRS” 2025).\n\n\nCode\n# Read in night light data tiles\ntile1_2021_02_07 &lt;- stars::read_stars(here::here(\"posts\", \"2025-12-01-eds223\", \"data\", \"VNP46A1\", \"VNP46A1.A2021038.h08v05.001.2021039064328.tif\"))\n\ntile2_2021_02_07 &lt;- stars::read_stars(here::here(\"posts\", \"2025-12-01-eds223\", \"data\", \"VNP46A1\", \"VNP46A1.A2021038.h08v06.001.2021039064329.tif\"))\n\ntile1_2021_02_16 &lt;- stars::read_stars(here::here(\"posts\", \"2025-12-01-eds223\", \"data\", \"VNP46A1\", \"VNP46A1.A2021047.h08v05.001.2021048091106.tif\"))\n\ntile2_2021_02_16 &lt;- stars::read_stars(here::here(\"posts\", \"2025-12-01-eds223\", \"data\", \"VNP46A1\", \"VNP46A1.A2021047.h08v06.001.2021048091105.tif\"))\n\n# Combine tiles for each date into a single continuous raster\nlights_2021_02_07 &lt;- st_mosaic(tile1_2021_02_07, tile2_2021_02_07)\n  \nlights_2021_02_16 &lt;- st_mosaic(tile1_2021_02_16, tile2_2021_02_16)\n\n# Crop each light raster to Houston area\nbb &lt;- st_bbox(c(xmin = -96.5, xmax = -94.5, ymax = 30.5, ymin = 29.0),\n              crs = st_crs(lights_2021_02_07))\n\nlights_2021_02_07 &lt;- st_crop(lights_2021_02_07, bb)\nlights_2021_02_16 &lt;- st_crop(lights_2021_02_16, bb)\n\n\n\nMap light differences\nTo visually compare the night light intensities before and after the first two storms, the VIIRS light rasters from February 7th (before) and February 16th (after) were mapped side by side.\n\n\nCode\n# Create manual breaks for clearer differences\nbreaks &lt;- c(0, 500, 1000, 1500, 2000, 2500, 3000)\n\n# Create plot of Feb 7th light raster- before storm\nbefore_lights &lt;- tm_shape(lights_2021_02_07) +\n  tm_raster(palette = \"-Greys\",\n            breaks = breaks,\n            title = \"Luminance (cm-2sr-1)\"\n    ) +\n  tm_title(text = \"Houston Area Luminance: Feb 7th, 2021\") +\n  tm_layout(legend.text.size = 0.5,\n            legend.title.size = 0.5) +\n  tm_graticules(alpha = 0.2)\n\n# Create plot of Feb 16th light raster- after storm\nafter_lights &lt;- tm_shape(lights_2021_02_16) +\n  tm_raster(palette = \"-Greys\",\n            breaks = breaks,\n            title = \"Luminance (cm-2sr-1)\"\n    ) +\n  tm_title(text = \"Houston Area Luminance: Feb 16th, 2021\") +\n  tm_layout(legend.text.size = 0.5,\n            legend.title.size = 0.5) +\n  tm_graticules(alpha = 0.2) \n\ntmap_arrange(before_lights, after_lights)\n\n\n\n\n\n\n\n\n\nThere are more lights visible in the date prior to the storms.\n\n\n\nB) Roads and Houses data\nThe road and house data was obtained from OpenStreetMap (OSM), a collaborative project which creates publicly available geographic data (“OpenStreetMap Geofabrik Download” 2025). The shapefiles were retrieved from Geofabrik’s download sites and then filtered to a Geopackage (.gpkg) file containing just the subset of features that intersect the Houston metropolitan area.\n\n\nCode\n# Read in roads data and filter to only class motorway\nroads &lt;- sf::st_read(here::here(\"posts/2025-12-01-eds223/data/gis_osm_roads_free_1.gpkg\"),\n                     query = \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\",\n                     quiet = TRUE)\n\n# Transform CRS to NAD83 / Texas Centric Albers Equal Area\nroads &lt;- st_transform(roads, 3083)\n\n# Read in buildings data and filter to only types of houses\nhouses &lt;- sf::st_read(here::here(\"posts/2025-12-01-eds223/data/gis_osm_buildings_a_free_1.gpkg\"),\n                      query = \"SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL)\nOR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\",\n                      quiet = TRUE)\n\n# Transform CRS to NAD83 / Texas Centric Albers Equal Area\nhouses &lt;- st_transform(houses, 3083)\n\n\n\n\nC) Socioeconomic attributes data and census tracts\nThe socioeconomic data for 2019 census tracts was obtained from the U.S Census Bureau’s American Community Survey, which collects detailed social, economic, housing, and demographic information from households across the US (U.S. Census Bureau 2025).\n\n\nCode\n# Read in entire geodatabase\nsocioeconomic &lt;- st_read(here::here(\"posts/2025-12-01-eds223/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\"), \n                         quiet = TRUE)\n\n# Explore socioeconomic geodatabase layers\nlayer_info &lt;- st_layers(dsn = here::here(\"posts/2025-12-01-eds223/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\"))\n#layer_info\n\n# Create geometry layer using appropriate layer names\nACS_tract_geometry &lt;- st_read(here::here(\"posts\", \"2025-12-01-eds223\", \"data\", \"ACS_2019_5YR_TRACT_48_TEXAS.gdb\"),\n                              layer = \"ACS_2019_5YR_TRACT_48_TEXAS\",\n                              quiet = TRUE)\n\n# Create attribute layer \nACS_tract_attributes &lt;- st_read(here::here(\"posts\", \"2025-12-01-eds223\", \"data\", \"ACS_2019_5YR_TRACT_48_TEXAS.gdb\"),\n                                layer = \"X19_INCOME\",\n                                quiet = TRUE)\n\n# Rename GEOID_Data column in ACS_tract_geometry to GEOID for a correct join\nACS_tract_geometry &lt;- ACS_tract_geometry %&gt;% \n  rename(GEOID_old = GEOID) %&gt;% \n  rename(GEOID = GEOID_Data)\n\n# Combine layers \nACS_tracts &lt;- ACS_tract_geometry %&gt;% \n  left_join(ACS_tract_attributes, by = \"GEOID\") %&gt;% \n  st_as_sf() \n\n# Crop to Houston area\nbb &lt;- st_bbox(c(xmin = -96.5, xmax = -94.5, ymax = 30.5, ymin = 29.0),\n              crs = st_crs(ACS_tracts))\n\nACS_tracts &lt;- st_crop(ACS_tracts, bb) \n\n# Transform CRS to NAD83 / Texas Centric Albers Equal Area\nACS_tracts &lt;- st_transform(ACS_tracts, 3083)\n\n\nAs a final data loading step, we want to ensure that the CRS of each data set is the same for further spatial operations. The CRS we are using for this analysis is the NAD83 / Texas Centric Albers Equal Area projection, or EPSG:3083.\n\n\nCode\n# Ensure data sets have same coordinate reference systems \nif((st_crs(roads) == st_crs(houses))\n   & (st_crs(houses) == st_crs(lights_2021_02_07)) \n   & (st_crs(lights_2021_02_07) == st_crs(lights_2021_02_16))\n   & (st_crs(lights_2021_02_07) == st_crs(ACS_tracts))){\n  print(\"Coordinate reference systems match\")\n} else{\n  warning(\" Updating coordinate reference systems to match\")\n  # transform data to match selected CRS\n  crs_3083 &lt;- st_crs(roads)\n  \n  roads &lt;- st_transform(roads, crs_3083)\n  lights_2021_02_07 &lt;- st_transform(lights_2021_02_07, crs_3083)\n  lights_2021_02_16 &lt;- st_transform(lights_2021_02_16, crs_3083)\n  ACS_tracts &lt;- st_transform(ACS_tracts, crs_3083)\n}\n\n\nWarning: Updating coordinate reference systems to match"
  },
  {
    "objectID": "posts/2025-12-01-eds223/index.html#find-locations-that-experienced-a-blackout",
    "href": "posts/2025-12-01-eds223/index.html#find-locations-that-experienced-a-blackout",
    "title": "Houston Power Grid Analysis",
    "section": "2. Find locations that experienced a blackout",
    "text": "2. Find locations that experienced a blackout\nTo identify places that experienced a blackout, a mask was created to indicate for each cell whether or not it experienced a blackout. The threshold for this mask was a drop of more than 200 nW cm-2sr-1 between the pre and post storm night light rasters. This blackout mask was then vectorized.\n\n\nCode\n# Find the change in night lights intensity pre and post storm\nlight_difference &lt;- lights_2021_02_07 - lights_2021_02_16\n\n# Assign NA to cells that experienced a change less than 200 nW cm-2sr-1\nlight_difference[light_difference &lt; 200] &lt;- NA\n\n# Vectorize the blackout mask and fix invalid geometries\nlight_diff_vec &lt;- light_difference %&gt;% \n  st_as_sf() %&gt;% \n  st_transform(3083)\n\n# Test for valid geometry\nexpect_false(any(!st_is_valid(light_diff_vec)))"
  },
  {
    "objectID": "posts/2025-12-01-eds223/index.html#exclude-highways-from-analysis",
    "href": "posts/2025-12-01-eds223/index.html#exclude-highways-from-analysis",
    "title": "Houston Power Grid Analysis",
    "section": "3. Exclude highways from analysis",
    "text": "3. Exclude highways from analysis\nStarting with the vectorized blackout mask, areas within 200m of a highway needed to be excluded as well from the analysis as these areas experience potentially confounding changes in light intensity unrelated to the storm.\n\n\nCode\n# Check units of CRS for distance argument in buffer\nunits &lt;- st_crs(roads)$units\n\n# Create 200m buffer around all highways\nhighway_buffer &lt;- st_buffer(roads, dist = 200) \n\n# Combine buffer geometries- dissolve multi-polygons\nhighway_union &lt;- st_union(highway_buffer) \n\n# Create data frame for blackouts that are not in the highway union\ntrue_blackouts &lt;- st_difference(light_diff_vec, highway_union)\n\n# Visualize buffer\ntm_shape(highway_union) +\n  tm_polygons(fill = \"#c38f16\") +\n  tm_title(text= \"200m highway buffer\") +\ntm_shape(true_blackouts) +\n  tm_polygons(fill = \"#122c43\") +\n  tm_basemap(\"CartoDB.PositronNoLabels\") +\n  tm_graticules(alpha = 0.2) \n\n\n\n\n\n\n\n\n\nThe units of the roads CRS is m."
  },
  {
    "objectID": "posts/2025-12-01-eds223/index.html#identify-homes-that-experienced-blackouts",
    "href": "posts/2025-12-01-eds223/index.html#identify-homes-that-experienced-blackouts",
    "title": "Houston Power Grid Analysis",
    "section": "4. Identify homes that experienced blackouts",
    "text": "4. Identify homes that experienced blackouts\nUsing the finalized vector data for areas that experienced blackouts from the previous analysis step, homes that experienced blackouts were identified by filtering the houses data to those that intersect with the true blackout polygons. The count of homes that lost power is therefore the number of rows in the resulting data frame.\n\n\nCode\n# Filter homes to those that intersect with the true blackouts polygon\nblackout_houses &lt;- st_intersection(houses, true_blackouts)\n\n# Get number of affected homes by number of rows\nn_affected_houses &lt;- nrow(blackout_houses)\n\n# Create map of homes that experienced blackouts\ntm_shape(blackout_houses) +\n  tm_polygons(fill = \"#122c43\") +\n  tm_add_legend(type = \"fill\",\n                labels = c(\"Blackout\"),\n                fill = c(\"#122c43\")) +\n  tm_basemap(\"CartoDB.PositronNoLabels\") +\n  tm_title(text = \"Houston Homes that Experienced Blackouts\") +\n  tm_graticules(alpha = 0.2)\n\n\n\n\n\n\n\n\n\nThe estimated number of homes in Houston that lost power between February 7th and February 16th, 2021 is 169469."
  },
  {
    "objectID": "posts/2025-12-01-eds223/index.html#identify-the-census-tracts-impacted-by-the-blackout",
    "href": "posts/2025-12-01-eds223/index.html#identify-the-census-tracts-impacted-by-the-blackout",
    "title": "Houston Power Grid Analysis",
    "section": "5. Identify the census tracts impacted by the blackout",
    "text": "5. Identify the census tracts impacted by the blackout\nTo identify the census tracts impacted by the blackout, the U.S Census tract data was filtered to only census tracts that spatially intersected homes that experienced blackouts. An additional column was added to retain this information, where TRUE corresponds to a tract that had homes that experienced a blackout and FALSE corresponds to those that did not. These census polygons were filtered based on this column and plotted in separate colors to visualize the impact in the Houston area by census tract.\n\n\nCode\n# Create df for tracts that intersect homes that experienced blackouts and create 'TRUE' column\nblackedout_tracts &lt;- st_filter(ACS_tracts, blackout_houses) %&gt;% \n  mutate(blackout = TRUE)\n\n# Define logic-based TRUE/FALSE column in full census data using previous data frame values\ncensus &lt;- ACS_tracts %&gt;% \n  mutate(blackout = ACS_tracts$GEOID %in% blackedout_tracts$GEOID)\n\n# Filter census data frame to reduce computation time\ncensus &lt;- census %&gt;% \n  select('GEOID', 'B19013e1', 'Shape', 'TRACTCE', 'blackout') \n\n# Select true blackouts for plotting\ncensus_true &lt;- census %&gt;% \n  filter(blackout == TRUE) \n\n# Plot full census tracts and overlay with plotted true census tracts\ntm_shape(census) +\n  tm_polygons(fill = \"#dce6e5\") +\ntm_shape(census_true) +\n  tm_polygons(fill = \"#6e948c\") +\n  tm_add_legend(type = \"fill\",\n                labels = c(\"Blackout\", \"No blackout\"),\n                fill = c(\"#6e948c\", \"#dce6e5\")) +\n  tm_layout(frame.double_line = TRUE) +\n  tm_basemap(\"CartoDB.PositronNoLabels\") +\n  tm_title(text = \"Houston Area Census Tracts Impacted by the Blackout\") +\n  tm_borders(lwd = 0.8) +\n  tm_graticules(alpha = 0.2)"
  },
  {
    "objectID": "posts/2025-12-01-eds223/index.html#distribution-of-median-household-income-and-blackout-impact",
    "href": "posts/2025-12-01-eds223/index.html#distribution-of-median-household-income-and-blackout-impact",
    "title": "Houston Power Grid Analysis",
    "section": "6. Distribution of median household income and blackout impact",
    "text": "6. Distribution of median household income and blackout impact\nAfter identifying which census tracts experienced blackouts during the winter storms and which did not, we can connect information about the estimated median income for each census tract. This will allow us to see whether or not there was a disproportionate effect.\n\n\nCode\n# Plot histogram of median income faceted by tracts affected and unaffected by the blackouts\nmedian_income_hist &lt;- ggplot(census, aes(x = B19013e1, fill = blackout)) +\n  geom_histogram(color = \"#484f4f\", bins = 20) +\n  scale_fill_manual(values = c(\"#dce6e5\",\"#6e948c\"),\n                    labels = c(\"Unaffected\", \"Affected\")) +\n  scale_x_continuous(labels = label_number(scale = 1e-3)) +\n  facet_wrap(~blackout, \n             labeller = as_labeller(c(`TRUE` = \"Affected\", `FALSE` = \"Unaffected\")) \n  ) +\n  labs(title = \"Median Household Income by Census Blackout Status\",\n    x = \"Median Household Income (thousands of USD)\",\n    y = \"Count\",\n    fill = \"Census Tract Status\"      \n  ) +\n  theme_minimal()\n\nmedian_income_hist\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create boxplot of median income faceted by tracts affected and unaffected by the blackouts\nmedian_income_box &lt;- ggplot(census, aes(x = blackout, y = B19013e1, fill = blackout)) +\n  geom_boxplot(alpha = 0.8) +\n  scale_fill_manual(values = c(\"#dce6e5\",\"#6e948c\"),\n                    labels = c(\"Unaffected\", \"Affected\")) +\n  labs(title = \"Median Household Income by Census Blackout Status\",\n       x = \"Blackout\",\n       y = \"Median Household Income (USD)\",\n       fill = \"Census Tract Status\") +\n  theme_minimal()\n\nmedian_income_box"
  },
  {
    "objectID": "posts/2025-12-01-eds223/index.html#interpretation",
    "href": "posts/2025-12-01-eds223/index.html#interpretation",
    "title": "Houston Power Grid Analysis",
    "section": "Interpretation",
    "text": "Interpretation\nBased upon the histogram and bar plot, census tracts that experienced blackouts during the winter storms had a slightly higher estimated median income with a wider range compared to census tracts that did not experience blackouts but there is not a definite disproportionate effect. Based upon the map, many of the census tracts that were affected are located in the metropolitan areas of Houston possibly due to the power grid there being more interconnected. That might have an effect on the income distributions if people living in urban areas have more varied incomes. One study limitation might have been how the blackouts were accounted for, as we don’t know for certain that these light differences were due to the storms and not other factors such as day of the week or not having as much activity in the city due to the severe weather conditions.\nFor further analysis, I would like to conduct the same analysis for rural Texas areas, and include additional time points to see how long different communities were without power. As a native Texan, I experienced this storm from Austin. The impacts were certainly felt across the state, and we experienced prolonged time without power. This crisis was also coming right after the COVID-19 pandemic, which also had extremely disproportionate impacts on marginalized communities in Texas (Ura and Garnham 2021)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AVA ROBILLARD",
    "section": "",
    "text": "Hi! I’m Ava, and an early career data scientist with a background in plant science. I’m always striving to learn and create in my work, and I especially value the growth that comes from working at the intersection of biological and technical fields. When I’m not busy coding, you can usually find me outside in the sunshine, curating my Pinterest boards and Spotify playlists to perfection, or possibly adopting another plant.\n\nEducationResearch ExperienceProfessional Experience\n\n\nMaster, Environmental Data Science (Expected June 2026) | Bren School of Environmental Science & Management | University of California, Santa Barbara (UCSB)\nBachelor of Science, Plant Sciences, minor in Data Science (May 2025) | Cornell University, Ithaca NY\n\n\nUndergraduate Research Assistant (2022-2025) | Phylogenetics of Non-model Plant Species | Specht Lab- Evolution of Plant Form and Function, Cornell University | Advisor: Jacob Landis, Ph.D.\nHerbarium Research Assistant (2023-2025) | Liberty Hyde Bailey Herbarium, Cornell University\nComputational Biology REU Intern (2024) | Project: Spatio-temporal Analysis of Urban Heat Island Effect Using Remote Sensing, GIS, and Google Earth Engine\n\n\nMaster’s Capstone Project (2026) | Rincon’s Bio Weaver Tool- From Disparate Data to Actionable Analysis | Clients: Rincon Consultants"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Herbarium Specimen Phenology\n\n\n\nMEDS\n\nR\n\nStatistics\n\n\n\nUsing logistic and segemented regression models to analyze shifts in leaf color timing\n\n\n\nAva Robillard\n\n\nDec 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nHouston Power Grid Analysis\n\n\n\nMEDS\n\nR\n\nGeospatial\n\n\n\nA geospatial exploration of Houston metropolitan area power outages caused by 2021 winter storms\n\n\n\nAva Robillard\n\n\nDec 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos Angeles Wildfires: False Color Imagery & Socioeconomic Analysis\n\n\n\nMEDS\n\nPython\n\nGeospatial\n\n\n\nUsing remote sensing and EJI data to analyze the 2025 Eaton and Palisades Fires\n\n\n\nAva Robillard\n\n\nDec 1, 2025\n\n\n\n\n\n\nNo matching items"
  }
]